{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d130f2c-5777-409e-a120-baa350a89d60",
   "metadata": {},
   "source": [
    "### Creating a Deep Neural Network from scratch (i.e. using only numpy) to determine whether a given object passing close to earth is hazardous or not.\n",
    "\n",
    "We will implement a binary classification neural network with two hidden layers and train it using a Kaggle dataset. The project will be divided into sections::\n",
    "\n",
    "- [0: Data preparation](#1)\n",
    "- [1: Initialisation of hyperparameters](#2)\n",
    "- [2: Forward propagation](#3)\n",
    "- [3: Back propagation](#4)\n",
    "- [4: Updating parameters](#5)\n",
    "- [5: Testing and evaluation](#6)\n",
    "\n",
    "We will use a Neural Network with two hidden layers, each consisting of five nodes. The activation functions are fixed to be LINEAR -> RELU -> RELU -> Sigmoid, although this can be easily generalised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9a324-01c2-4541-99a1-11a26f6e2350",
   "metadata": {},
   "source": [
    "## 0: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9da25-2d46-4f5d-b705-0a672b5ada44",
   "metadata": {},
   "source": [
    "We will use the Kaggle dataset 'nearest-earth-objects' which contains data on 300000+ near-earth objects, including a label which tells us if each object is hazardous or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b09e646a-fcfa-4693-8ed7-58e61d697c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "import math\n",
    "import numpy as np   # for various mathematical operations (e.g. forward and back proagation)\n",
    "import pandas as pd  # for processing the dataset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35636f81-c823-4cc2-bbeb-f82a6ec06d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv('nearest-earth-objects.csv')\n",
    "\n",
    "# Naming the columns that are irrelevant for training the NN\n",
    "columns_to_drop = ['neo_id', 'name', 'orbiting_body', 'is_hazardous'] \n",
    "\n",
    "# Dropping the specified columns\n",
    "ds = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Preparing the labels\n",
    "dy = df['is_hazardous'].values\n",
    "dy = np.array(dy, dtype = int)\n",
    "\n",
    "# Prepare the training, validation and test sets\n",
    "training_set_x = ds.head(50000).T\n",
    "validation_set_x = ds.loc[270401: 304200].T\n",
    "test_set_x = ds.loc[304201 : 338000].T\n",
    "\n",
    "# Prepare the labels for the training, validation and test sets\n",
    "training_set_y = dy[:50000]   # We choose the first 50000 elements to form the training set.\n",
    "validation_set_y = dy[270401:304201]\n",
    "test_set_y = dy[304201 : 338001]\n",
    "\n",
    "training_set_y = training_set_y.reshape(1,training_set_y.shape[0])\n",
    "test_set_y = test_set_y.reshape(1,test_set_y.shape[0])\n",
    "validation_set_y = validation_set_y.reshape(1,validation_set_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7500f8-3628-469b-a889-18ccdcbcb337",
   "metadata": {},
   "source": [
    "Let's visualise the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0c5be56-2486-4f96-bc24-d764c2333bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               0             1             2      \\\n",
      "absolute_magnitude      1.914000e+01  1.850000e+01  2.145000e+01   \n",
      "estimated_diameter_min  3.949617e-01  5.303407e-01  1.363186e-01   \n",
      "estimated_diameter_max  8.831612e-01  1.185878e+00  3.048176e-01   \n",
      "relative_velocity       7.174540e+04  1.099498e+05  2.486551e+04   \n",
      "miss_distance           5.814362e+07  5.580105e+07  6.720689e+07   \n",
      "\n",
      "                               3             4             5      \\\n",
      "absolute_magnitude      2.063000e+01  2.270000e+01  2.500000e+01   \n",
      "estimated_diameter_min  1.988635e-01  7.665756e-02  2.658000e-02   \n",
      "estimated_diameter_max  4.446722e-01  1.714115e-01  5.943469e-02   \n",
      "relative_velocity       7.889008e+04  5.603652e+04  4.747765e+04   \n",
      "miss_distance           3.039644e+07  6.311863e+07  4.290521e+07   \n",
      "\n",
      "                               6             7             8      \\\n",
      "absolute_magnitude      2.150000e+01  1.975000e+01  2.170000e+01   \n",
      "estimated_diameter_min  1.332156e-01  2.982325e-01  1.214940e-01   \n",
      "estimated_diameter_max  2.978791e-01  6.668682e-01  2.716689e-01   \n",
      "relative_velocity       5.785330e+04  4.697249e+04  3.742455e+04   \n",
      "miss_distance           2.727908e+07  6.997593e+07  5.657727e+06   \n",
      "\n",
      "                               9      ...         49990         49991  \\\n",
      "absolute_magnitude      2.345000e+01  ...  2.033000e+01  2.640000e+01   \n",
      "estimated_diameter_min  5.426939e-02  ...  2.283258e-01  1.394938e-02   \n",
      "estimated_diameter_max  1.213501e-01  ...  5.105520e-01  3.119177e-02   \n",
      "relative_velocity       3.852487e+04  ...  3.579171e+04  4.998478e+04   \n",
      "miss_distance           6.166118e+07  ...  4.739660e+07  5.273383e+07   \n",
      "\n",
      "                               49992         49993         49994  \\\n",
      "absolute_magnitude      2.230000e+01  2.830000e+01  1.981000e+01   \n",
      "estimated_diameter_min  9.216265e-02  5.815070e-03  2.901048e-01   \n",
      "estimated_diameter_max  2.060820e-01  1.300289e-02  6.486941e-01   \n",
      "relative_velocity       8.448556e+04  6.009079e+04  8.601167e+04   \n",
      "miss_distance           4.129924e+07  5.642140e+07  4.168001e+07   \n",
      "\n",
      "                               49995         49996         49997  \\\n",
      "absolute_magnitude      1.921000e+01  2.766000e+01  1.793000e+01   \n",
      "estimated_diameter_min  3.824327e-01  7.808273e-03  6.895329e-01   \n",
      "estimated_diameter_max  8.551454e-01  1.745983e-02  1.541842e+00   \n",
      "relative_velocity       4.377326e+04  8.113932e+04  9.927099e+04   \n",
      "miss_distance           6.696717e+07  4.323995e+06  5.067816e+07   \n",
      "\n",
      "                               49998         49999  \n",
      "absolute_magnitude      2.639000e+01  2.362000e+01  \n",
      "estimated_diameter_min  1.401377e-02  5.018281e-02  \n",
      "estimated_diameter_max  3.133574e-02  1.122122e-01  \n",
      "relative_velocity       3.088250e+04  1.908142e+04  \n",
      "miss_distance           4.144248e+07  1.601976e+07  \n",
      "\n",
      "[5 rows x 50000 columns]\n"
     ]
    }
   ],
   "source": [
    "print(training_set_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2907e-33c0-4ac7-81bf-181cf90a2c07",
   "metadata": {},
   "source": [
    "Next, we define the activation functions that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "739afb70-ec2a-4148-aa28-875c2e077898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class which contains the activation functions we will use\n",
    "\n",
    "class Activation():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass  ## Nothing to initialise\n",
    "\n",
    "    @staticmethod  # We use a static method since there is no need to reference self.\n",
    "    def sigmoid(x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1/ (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(x, alpha=0.01):\n",
    "        return np.where(x > 0, x, alpha * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb1390-e510-4a3e-b6c7-30f5be0e386a",
   "metadata": {},
   "source": [
    "## 1: Initialisation\n",
    "We create a function initialize_parameters which generates the weights for each layer in the network. We use the np.random function and multiply by 0.01 to 'normalise' the data for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "795af26c-3f9a-4360-a9b8-5c0228f37ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the hyperparameters\n",
    "\n",
    "L = 2  # Number of hidden layers\n",
    "depth_layers = (training_set_x.shape[0], 5, 5, 1) # Dimensions of layers\n",
    "\n",
    "def initialize_parameters(n_x, n_h):\n",
    "    # INPUTS\n",
    "    # n_x = size of input layer (array)\n",
    "    # layers_h = sizes of hidden layers (array)\n",
    "    # n_y = size of output layer \n",
    "    \n",
    "    # OUTPUTS\n",
    "    # dictionary with weights Wi and bi for i = 1, ..., L    \n",
    "\n",
    "    weights = {}\n",
    "    for i in range(1, len(n_h)):\n",
    "        #w_name = f'W{i}'\n",
    "        weights[\"W\" + str(i)] = np.random.randn(n_h[i], n_h[i-1]) * 0.1\n",
    "\n",
    "        #b_name = f'b{i}'\n",
    "        weights[\"b\" + str(i)] = np.zeros((n_h[i], 1))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad89e7-6be7-486f-b4fb-6559a81c020c",
   "metadata": {},
   "source": [
    "## 2: Forward Propagation\n",
    "Now let's write the function that performs the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6bda95ad-2293-4e07-bbb6-65422dcca1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we create a function which performs a forward pass over one layer\n",
    "\n",
    "activation_fns = [Activation.leaky_relu, Activation.leaky_relu, Activation.leaky_relu, Activation.sigmoid]\n",
    "\n",
    "weights = initialize_parameters(training_set_x.shape[0], depth_layers)\n",
    "\n",
    "def forward(activations, weights, X):\n",
    "    ## INPUTS:\n",
    "    # activations: an array which contains the activation functions we will use (in order)\n",
    "    # weights:  dictionary containing the weights Wi and bi\n",
    "\n",
    "    ## OUTPUTS:\n",
    "    # cache: an array containing the outputs (with/without activation) after each layer\n",
    "    # cache = [Z0, Z1, A1, Z2, A2, Z3, A3]\n",
    "    cache = {\"A0\" : X}\n",
    "    \n",
    "    for i in range(1, len(depth_layers)):\n",
    "        #Wi = weights[\"W\" + str(i)]\n",
    "        cache[\"Z\" + str(i)] = np.dot(weights[\"W\" + str(i)], cache[\"A\" + str(i-1)]) + weights[\"b\" + str(i)]\n",
    "        cache[\"A\" + str(i)] = activation_fns[i](cache[\"Z\"+str(i)])\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cc98e-77f3-448f-bfcf-9c79a621ce07",
   "metadata": {},
   "source": [
    "We will also need a function that computes the cost. For the cost function, we choose the cross-entropy cost $J$, defined as $$ -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "425d8973-6b33-40c5-8ecd-faed4c2880af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    ## INPUTS:\n",
    "    # AL -- vector corresponding to label predictions, shape (1, number of examples)\n",
    "    # Y -- true \"label\" vector (for example: containing 0 if non-hazardous, 1 if hazardous), shape (1, number of examples)\n",
    "\n",
    "    # OUTPUTS:\n",
    "    # cost -- cross-entropy cost\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    k = 1e-10  # Add k to avoid log(0) errors occuring\n",
    "    cost = - (np.sum( Y * np.log(AL + k) + (1-Y) * np.log(1-AL +k), axis=1, keepdims = True)) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure the cost's shape is as expected (e.g. it turns [[7]] into 7).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f32222-d594-44d0-b864-25f612c303dd",
   "metadata": {},
   "source": [
    "## 3: Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe45c2-b476-40a2-a4c7-b9054e2b541f",
   "metadata": {},
   "source": [
    "In this step, we calculate the gradients of our loss function with respect to the activations, weights and biases of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bca11-b15a-4e02-89a5-7c599d4bf20f",
   "metadata": {},
   "source": [
    "For each $l$, the three gradients $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\r\n",
    "Here are the expressions for the gradients:d:\r\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\r\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\r\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\r\n",
    "\r\n",
    "\r\n",
    "$A^{[l-1] T}$ is the transpose of $A^{[l-1]}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ece87-84b9-48d4-b59b-bca45631c19d",
   "metadata": {},
   "source": [
    "Of course, to use these formulas we will first need to compute $dZ^{[l]}$ for each layer $l$. This is done in the backward function below and stored in a dictionary labelled dZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d466b55-d22d-43f0-b800-4cffb24536d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we perform the backpropagation\n",
    "# We will need the derivatives of our activation functions, so let's define those first.\n",
    "\n",
    "def relu_grad(x):\n",
    "        return (x > 0) * 1\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "        return Activation.sigmoid(x)*(1-Activation.sigmoid(x))\n",
    "\n",
    "def linear_grad(x):\n",
    "    return np.ones_like(x)\n",
    "        \n",
    "# Now we define a function which does backprop over one layer.\n",
    "def backward(cache, Y, weights):\n",
    "    ## INPUTS:\n",
    "    # cache: a dictionary containing all the weights\n",
    "\n",
    "    ## OUTPUTS:\n",
    "    # grads: a dictionary containing all the gradients\n",
    "    \n",
    "    m = Y.shape[1]  # letting m be the number of training examples\n",
    "\n",
    "    # Creating the dictionary dZ which will store the gradients dL/Dz^i\n",
    "    dZ = {\"dZ3\" : cache[\"A\" + str(3)] - Y}\n",
    "    dZ[\"dZ2\"] = np.multiply(np.dot(weights[\"W\" + str(3)].T, dZ[\"dZ3\"]), relu_grad(cache[\"Z\" + str(2)]))\n",
    "    dZ[\"dZ1\"] = np.dot(weights[\"W\" + str(2)].T, dZ[\"dZ2\"])\n",
    "    #dZ[\"dZ1\"] = np.multiply(np.dot(weights[\"W\" + str(2)].T, dZ[\"dZ2\"]), linear_grad(cache[\"Z\" + str(1)]))\n",
    "\n",
    "\n",
    "    # Y1 and Y2 appear in the cost function, so we define them first for more readable computations.\n",
    "    Y1 = np.multiply(Y, 1/ (cache[\"A\" + str(3)]))\n",
    "    Y2 = np.multiply((1-Y), 1 / (cache[\"A\" + str(3)]))\n",
    "    \n",
    "    # Now we calculate the gradients for each layer using the formulas above.\n",
    "    grads = {}\n",
    "    for i in range (1, len(depth_layers)):\n",
    "        grads[\"dW\" + str(i)] = ( np.dot(dZ[\"dZ\" + str(i)], cache[\"A\" + str(i-1)].T ) ) / m\n",
    "        grads[\"db\" + str(i)] = ( np.sum(dZ[\"dZ\" + str(i)], axis=1, keepdims = True) ) / m\n",
    "        grads[\"dA\" + str(i-1)] = ( np.dot(weights[\"W\" + str(i)].T, dZ[\"dZ\" + str(i)]))\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387c3b9-0114-4b05-9c29-cacd0508b0c2",
   "metadata": {},
   "source": [
    "## 4: Updating the parameters\n",
    "We write a function which updates the parameters using a standard gradient descent approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7c6c8-e33c-4b9c-b144-102dbdb19fcb",
   "metadata": {},
   "source": [
    "Recall that the update rule for gradient descent reads as $$\\theta_{k+1} = \\theta_{k} - \\alpha \\nabla \\mathcal{J}(\\theta_{k}) $$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "032a6387-adbb-4057-ab92-9f625dbfbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    parameters = copy.deepcopy(params)\n",
    "    N = len(parameters)\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(3):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5d39da9-7ef2-4576-b6d6-860f165801a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, depth_layers, learning_rate = 0.001, num_iterations=2000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a 2-layer neural network: [RELU]*(L-1)->SIGMOID.\n",
    "    \n",
    "    INPUTS:\n",
    "    X -- input data\n",
    "    Y -- true label vector of shape (1, number of examples)\n",
    "    depth_layers -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate for the gradient descent update\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    OUTPUTS:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    weights = initialize_parameters(training_set_x.shape[0], depth_layers)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        cache = forward(activation_fns, weights, training_set_x)  # Forward pass, returning cache an array of outputs [Z0, A0, ... , Z3 ,A3]\n",
    "        cost = compute_cost(cache[\"A\" + str(3)], Y)  # Computing the cost\n",
    "        grads = backward(cache, Y, weights) # Backward pass, returning grads [dWi, dbi, dAi for i = 1,2,3]\n",
    "        weights = update_parameters(weights, grads, learning_rate)  # Updating the cache array with new parameters.\n",
    "\n",
    "        #if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "        #    print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        #if i % 100 == 0 or i == num_iterations:\n",
    "        costs.append(cost)\n",
    "    \n",
    "    return cache, costs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a96846-37a9-4088-aee0-390c1f7bbf8d",
   "metadata": {},
   "source": [
    "# 5: Testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a14aab-199a-4fce-98b5-894c856eebe4",
   "metadata": {},
   "source": [
    "First, we train the model for 500 iterations and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "775c3286-1711-4981-bb28-8e75ef1a9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Class Distribution: {0.0: 42009, 1.0: 42009}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine features and labels for easy resampling\n",
    "train_data = np.hstack((training_set_x.T, training_set_y.T))  # Convert to shape (N, features + 1)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority = train_data[train_data[:, -1] == 0]  # Non-hazardous\n",
    "minority = train_data[train_data[:, -1] == 1]  # Hazardous\n",
    "\n",
    "# Oversample minority class to match majority size\n",
    "minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
    "\n",
    "# Combine back into one dataset\n",
    "balanced_data = np.vstack((majority, minority_upsampled))\n",
    "np.random.shuffle(balanced_data)  # Shuffle to mix 0s and 1s\n",
    "\n",
    "# Separate back into X and Y\n",
    "training_set_x = balanced_data[:, :-1].T  # Features\n",
    "training_set_y = balanced_data[:, -1].reshape(1, -1)  # Labels\n",
    "\n",
    "# Check new class balance\n",
    "unique, counts = np.unique(training_set_y, return_counts=True)\n",
    "print(f\"Balanced Training Class Distribution: {dict(zip(unique, counts))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa9fb6-5c87-42c6-843c-93b018a197c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 500\n",
    "\n",
    "parameters, costs, weights = two_layer_model(training_set_x, training_set_y, (training_set_x.shape[0], 5, 5, 1), learning_rate = 0.05, num_iterations=N)\n",
    "\n",
    "\n",
    "def plot_costs(costs, N, window=10):\n",
    "    \"\"\"\n",
    "    Plots the evolution of cost with iterations, including a smoothed curve.\n",
    "    \"\"\"\n",
    "    x_range = np.arange(1, N + 1)\n",
    "\n",
    "    # Compute moving average for smoothing\n",
    "    smoothed_costs = np.convolve(costs, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(x_range, costs, label=\"Raw Cost\", alpha=0.5)\n",
    "    plt.plot(x_range[:len(smoothed_costs)], smoothed_costs, label=f\"Smoothed (window={window})\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Training Cost vs. Iterations')\n",
    "    plt.yscale(\"log\")  # Log scale for better visibility\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_costs(costs, N)\n",
    "\n",
    "# Let's also evaluate the model on the training and test sets.\n",
    "def predict(weights, X):\n",
    "    \"\"\"\n",
    "    Carries out the forward propagation and returns binary predictions.\n",
    "    \"\"\"\n",
    "    cache = forward(activation_fns, weights, X)\n",
    "    return (cache[\"A3\"] > 0.5).astype(int)\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return np.mean(predictions == labels) * 100\n",
    "\n",
    "def predict_with_threshold(weights, X, threshold=0.2):\n",
    "    cache = forward(activation_fns, weights, X)\n",
    "    return (cache[\"A3\"] > threshold).astype(int)\n",
    "\n",
    "test_predictions = predict_with_threshold(weights, test_set_x, threshold=0.3)\n",
    "\n",
    "# Predictions\n",
    "train_predictions = predict(weights, training_set_x)\n",
    "test_predictions = predict(weights, test_set_x)\n",
    "\n",
    "# Accuracy\n",
    "train_acc = compute_accuracy(train_predictions, training_set_y)\n",
    "test_acc = compute_accuracy(test_predictions, test_set_y)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3b544-751f-40cb-ade9-879dd9fe141d",
   "metadata": {},
   "source": [
    "For imbalanced datasets, the accuracy on the training and test sets may not suffice, so we also calculate precision, recall and the F score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "31f24a75-be9d-480e-9d2a-5c66ab7964c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07\n",
      "Recall: 1.00\n",
      "F1 Score: 0.13\n",
      "Prediction Distribution: {0: 2, 1: 33798}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert predictions to 1D arrays\n",
    "train_preds_flat = train_predictions.flatten()\n",
    "test_preds_flat = test_predictions.flatten()\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(test_set_y.flatten(), test_preds_flat)\n",
    "recall = recall_score(test_set_y.flatten(), test_preds_flat)\n",
    "f1 = f1_score(test_set_y.flatten(), test_preds_flat)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "unique, counts = np.unique(test_preds_flat, return_counts=True)\n",
    "print(f\"Prediction Distribution: {dict(zip(unique, counts))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513ab21-1ef6-4d68-8b0a-61cbb3ed6029",
   "metadata": {},
   "source": [
    "Let's wrap it up by obtaining the confusion matrix."
   ]
 
